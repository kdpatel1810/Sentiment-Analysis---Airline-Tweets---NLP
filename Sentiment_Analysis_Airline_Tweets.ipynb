{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tb6-T8p0mQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8909e388-3e08-4127-b446-bd679e87d8f4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string \n",
        "!pip install more-itertools\n",
        "import itertools\n",
        "import collections"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (8.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyq_MYkZxgSC",
        "colab_type": "text"
      },
      "source": [
        "### Load : Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOJN0bCm1GFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "cc3c7133-27e3-4845-c500-24f09e0a1d84"
      },
      "source": [
        "train_data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/NLP/HW1/train/train1.csv\")\n",
        "train_data.info()\n",
        "train_data.describe()\n",
        "text_train, y_train = train_data['text'], train_data['Target']\n",
        "target_catagory_counts = y_train.value_counts()\n",
        "print(target_catagory_counts)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7320 entries, 0 to 7319\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Id      7320 non-null   int64 \n",
            " 1   text    7320 non-null   object\n",
            " 2   Target  7320 non-null   int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 171.7+ KB\n",
            "-1    4566\n",
            " 0    1536\n",
            " 1    1218\n",
            "Name: Target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IZqbIHny4Zb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "5b87415e-46dc-4390-a3aa-da7bee222c07"
      },
      "source": [
        "text_train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       @USAirways  ! THE WORST in customer service. @...\n",
              "1       @united call wait times are over 20 minutes an...\n",
              "2       @JetBlue what's up with the random delay on fl...\n",
              "3       @AmericanAir Good morning!  Wondering why my p...\n",
              "4       @united UA 746. Pacific Rim and Date Night cut...\n",
              "                              ...                        \n",
              "7315                              @AmericanAir followback\n",
              "7316    @united thanks for the help. Wish the phone re...\n",
              "7317    @usairways the. Worst. Ever. #dca #customerser...\n",
              "7318    @nrhodes85: look! Another apology. DO NOT FLY ...\n",
              "7319    @united you are by far the worst airline. 4 pl...\n",
              "Name: text, Length: 7320, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_nhv-ZU1Uyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "06f87eb9-25fe-4c2e-89c1-bd498eb57fc5"
      },
      "source": [
        "test_data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/NLP/HW1/train/test1.csv\")\n",
        "test_data.info()\n",
        "test_data.describe()\n",
        "text_test = test_data['text']"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7320 entries, 0 to 7319\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      7320 non-null   int64 \n",
            " 1   text    7320 non-null   object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 114.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx_QEh4izI47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "93868103-d4a9-4577-b712-cd6335a93a8b"
      },
      "source": [
        "text_test"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       @AmericanAir In car gng to DFW. Pulled over 1h...\n",
              "1       @AmericanAir after all, the plane didnÂ‰Ã›Âªt ...\n",
              "2       @SouthwestAir can't believe how many paying cu...\n",
              "3       @USAirways I can legitimately say that I would...\n",
              "4       @AmericanAir still no response from AA. great ...\n",
              "                              ...                        \n",
              "7315    @JetBlue Traveling with two kids tomorrow (age...\n",
              "7316    @JetBlue Tx for the info. Just don't understan...\n",
              "7317    @AmericanAir I understand. But why is this the...\n",
              "7318                                 @USAirways really!??\n",
              "7319    @united no I did not make connection.  Your st...\n",
              "Name: text, Length: 7320, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KheyzO8x0GO",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPKoJtv2x66t",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenizer, Stemmer, Lemmatizer and Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaF53JngGnMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "48ebd8d3-8bc0-40b3-c649-ddda001f09f5"
      },
      "source": [
        "## Remove Non ascii characters from tweet\n",
        "text_train = text_train.str.encode('ascii', 'ignore').str.decode('ascii')\n",
        "\n",
        "## Remove Tweeter handlers from the tweet and HTTP URLs\n",
        "text_train = text_train.str.replace('@[A-Za-z0-9]+','')\n",
        "text_train = text_train.str.replace('https?://[A-Za-z0-9./]+','')\n",
        "\n",
        "##Remove only numeric tokens\n",
        "text_train = text_train.str.replace('[0-9]+','')\n",
        "\n",
        "##sentence Tokenizer\n",
        "#from nltk import sent_tokenize\n",
        "#text_train = text_train.apply(lambda row: sent_tokenize(row.lower()))\n",
        "\n",
        "## Remove pactuations from the tweet words\n",
        "text_train = text_train.str.replace('[{}]'.format(string.punctuation),'')\n",
        "\n",
        "## Lower case & Word Tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "text_train = text_train.apply(lambda row: word_tokenize(row.lower()))\n",
        "\n",
        "##Stemming\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "def stemming(data):\n",
        "  out_sentence = [PorterStemmer().stem(token) for token in data]\n",
        "  return out_sentence\n",
        "text_train = text_train.apply(lambda row: stemming(row))\n",
        "\n",
        "##lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "def lemmatizer(data):\n",
        "  lemmatizer = WordNetLemmatizer()  \n",
        "  out_sentence = [lemmatizer.lemmatize(token) for token in data]\n",
        "  return(out_sentence)\n",
        "text_train = text_train.apply(lambda row: lemmatizer(row))\n",
        "\n",
        "## Remove Stop Words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "def stopword(data):\n",
        "  stop = set(nltk_stopwords.words('english'))\n",
        "  wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "  for word in wh_words: \n",
        "      stop.remove(word)\n",
        "  out_sentence = \" \".join([token for token in data if token not in stop])\n",
        "  return(out_sentence)\n",
        "text_train = text_train.apply(lambda row: stopword(row))\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAfdPJnVzmE5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "318716c4-bf0b-4824-be9f-d2b2ecfe0b09"
      },
      "source": [
        "text_train"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       worst custom servic call month book flight poo...\n",
              "1           call wait time minut airport wait time longer\n",
              "2           what random delay flight ani chanc fals alarm\n",
              "3       good morn wonder whi pretsa check wa board pas...\n",
              "4       ua pacif rim date night cut constantli randoml...\n",
              "                              ...                        \n",
              "7315                                           followback\n",
              "7316              thank help wish phone rep could accomid\n",
              "7317                        worst ever dca customerservic\n",
              "7318                                look anoth apolog fli\n",
              "7319    far worst airlin plane delay round trip flight...\n",
              "Name: text, Length: 7320, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIgWZPshqFIA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8e139721-d31f-407d-f8c0-362069a10182"
      },
      "source": [
        "## Remove Non ascii characters from tweet\n",
        "text_test = text_test.str.encode('ascii', 'ignore').str.decode('ascii')\n",
        "\n",
        "## Remove Tweeter handlers from the tweet and HTTP URLs\n",
        "text_test = text_test.str.replace('@[A-Za-z0-9]+','')\n",
        "text_test = text_test.str.replace('https?://[A-Za-z0-9./]+','')\n",
        "\n",
        "##Remove only numeric tokens\n",
        "text_test = text_test.str.replace('[0-9]+','')\n",
        "\n",
        "##sentence Tokenizer\n",
        "#from nltk import sent_tokenize\n",
        "#text_test = text_test.apply(lambda row: sent_tokenize(row.lower()))\n",
        "\n",
        "## Remove pactuations from the tweet words\n",
        "text_test = text_test.str.replace('[{}]'.format(string.punctuation),'')\n",
        "\n",
        "## Lower case & Word Tokenizer\n",
        "from nltk import word_tokenize\n",
        "text_test = text_test.apply(lambda row: word_tokenize(row.lower()))\n",
        "\n",
        "##Stemming\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "def stemming(data):\n",
        "  out_sentence = [PorterStemmer().stem(token) for token in data]\n",
        "  return out_sentence\n",
        "text_test = text_test.apply(lambda row: stemming(row))\n",
        "\n",
        "##lemmatizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "def lemmatizer(data):\n",
        "  lemmatizer = WordNetLemmatizer()  \n",
        "  out_sentence = [lemmatizer.lemmatize(token) for token in data]\n",
        "  return(out_sentence)\n",
        "text_test = text_test.apply(lambda row: lemmatizer(row))\n",
        "\n",
        "## Remove Stop Words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "def stopword(data):\n",
        "  stop = set(nltk_stopwords.words('english'))\n",
        "  wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "  for word in wh_words: \n",
        "      stop.remove(word)\n",
        "  out_sentence = \" \".join([token for token in data if token not in stop])\n",
        "  return(out_sentence)\n",
        "text_test = text_test.apply(lambda row: stopword(row))\n",
        "\n",
        "## Remove in frequent words and too many frequent words \n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRtLR65Aqj-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "31a1639f-4e90-4dc2-d5a2-8f1a41d1a3a8"
      },
      "source": [
        "text_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       car gng dfw pull hr ago veri ici road onhold a...\n",
              "1       plane didnt land ident wors condit grk accord ...\n",
              "2       cant believ how mani pay custom left high dri ...\n",
              "3       legitim say would rather driven cross countri ...\n",
              "4                          still respons aa great job guy\n",
              "                              ...                        \n",
              "7315    travel two kid tomorrow age domest need birth ...\n",
              "7316    tx info dont understand whi couldnt accur esti...\n",
              "7317    understand whi thi onli flight day go twice im...\n",
              "7318                                               realli\n",
              "7319    make connect stellar employe vicki thoma refus...\n",
              "Name: text, Length: 7320, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWU55MBZxeLM",
        "colab_type": "text"
      },
      "source": [
        "#### Feature Selection : CountVectorizer, TfidfVectorizer, Bag of Word, HashingVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo_wzujtDtDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ba1e23e-05ce-4f91-c81e-12ef2b3d0a6a"
      },
      "source": [
        "#### CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "Cvect = CountVectorizer(analyzer='word', ngram_range=(1,2))\n",
        "X_train_cvect= Cvect.fit_transform(text_train)\n",
        "X_test_cvect = Cvect.transform(text_test)\n",
        "#print(Cvect.get_feature_names())\n",
        "print(len(Cvect.get_feature_names()))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K499lWqn1f7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Feature selection - Bag of Word\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bowvect = CountVectorizer(analyzer='word')\n",
        "X_train_bow = bowvect.fit_transform(text_train).toarray()\n",
        "X_test_bow = bowvect.transform(text_test).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iI-eGw7Dzb-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5009fca7-3826-41c8-9941-2aedc9a574f9"
      },
      "source": [
        "#### TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "Tvect = TfidfVectorizer(analyzer='word')\n",
        "X_train_tvect = Tvect.fit_transform(text_train)\n",
        "X_test_tvect = Tvect.transform(text_test)\n",
        "#print(Tvect.get_feature_names())\n",
        "print(len(Tvect.get_feature_names()))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p8vSrf5E-Ox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "12fb951b-e6e5-4323-f272-b91bc2f45889"
      },
      "source": [
        "### HashingVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "hvect = HashingVectorizer(analyzer='word', ngram_range=(1, 2), n_features=30000)\n",
        "X_train_hvect = hvect.fit_transform(text_train)\n",
        "X_test_hvect = hvect.transform(text_test)\n",
        "print(X_train_hvect.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7320, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we3rpLWsF0AF",
        "colab_type": "text"
      },
      "source": [
        "### ML Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1uGXQyfM2l6",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-B6wGhKH7xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "param_grid = {'C': [0.8,0.9,1.0,1.1,1.2]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200,penalty= 'l2'), param_grid, cv=5, scoring='f1_macro')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbQ6AXuwTC2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Save results in list\n",
        "amsl={'PreProcessing':[],'ML_Model':[],'Mean_Score':[]}\n",
        "allmodelscorelist = pd.DataFrame(amsl)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYogzG5vIm0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f0310679-b357-4c32-ad47-ba6a8f569dae"
      },
      "source": [
        "## CountVectorizer\n",
        "lg_cv = grid.fit(X_train_cvect, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'CountVectorizer','ML_Model':'LogisticRegression','Mean_Score':grid.best_score_}, ignore_index=True) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.7163\n",
            "Best parameters:  {'C': 1.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWd-TQN8IlEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e388b172-3911-4880-9440-374176edf2ec"
      },
      "source": [
        "## TfidfVectorizer\n",
        "lg_tf = grid.fit(X_train_tvect, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'TfidfVectorizer','ML_Model':'LogisticRegression','Mean_Score':grid.best_score_}, ignore_index=True) \n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.6810\n",
            "Best parameters:  {'C': 1.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdjmc5gqLVfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2d585a90-18ec-4c05-d279-e0fee652cc1d"
      },
      "source": [
        "## Bag of Word\n",
        "lg_bow = grid.fit(X_train_bow, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'Bag of Word','ML_Model':'LogisticRegression','Mean_Score':grid.best_score_}, ignore_index=True) \n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.7163\n",
            "Best parameters:  {'C': 1.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVTaQMe3MmSq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9b62f9d4-05ab-4ed7-c194-2daf1debb31f"
      },
      "source": [
        "## HashingVectorizer\n",
        "lg_hv = grid.fit(X_train_hvect, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'HashingVectorizer','ML_Model':'LogisticRegression','Mean_Score':grid.best_score_}, ignore_index=True) \n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.6648\n",
            "Best parameters:  {'C': 1.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJxm1yOlM7Xq",
        "colab_type": "text"
      },
      "source": [
        "#### XGBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykjPTXnY27y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "param_grid = {'learning_rate': [0.1,0.2]}\n",
        "grid = GridSearchCV(XGBClassifier(n_estimators=400, max_depth=5, min_child_weight=1, gamma=0.1, subsample=0.8, colsample_bytree=0.8, objective='multi:softmax', num_class=3, scale_pos_weight = 5), param_grid, cv=5, scoring='f1_macro')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m-Xj9NaKNYMB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c189f7c2-d22f-45da-e962-269b071fe9d3"
      },
      "source": [
        "## CountVectorizer\n",
        "xg_cv = grid.fit(X_train_cvect, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'CountVectorizer','ML_Model':'XGBoostClassifier','Mean_Score':grid.best_score_}, ignore_index=True) "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.7151\n",
            "Best parameters:  {'learning_rate': 0.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7rt2IWzPNf0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f1c27193-efaf-4944-c099-fbea63c8cfff"
      },
      "source": [
        "## TfidfVectorizer\n",
        "xg_tf = grid.fit(X_train_tvect, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'TfidfVectorizer','ML_Model':'XGBoostClassifier','Mean_Score':grid.best_score_}, ignore_index=True) \n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.6897\n",
            "Best parameters:  {'learning_rate': 0.2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qpxOKG2nR21Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7355c324-00c6-48b1-9891-4650c8529ca4"
      },
      "source": [
        "## Bag of Word\n",
        "xg_bow = grid.fit(X_train_bow, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'Bag of Word','ML_Model':'XGBoostClassifier','Mean_Score':grid.best_score_}, ignore_index=True) "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.6991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SME4KuU5RnZR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2efc10bc-8372-4da5-f9ff-2a1ad4c255e7"
      },
      "source": [
        "## HashingVectorizer\n",
        "xg_hv = grid.fit(X_train_hvect, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'HashingVectorizer','ML_Model':'XGBoostClassifier','Mean_Score':grid.best_score_}, ignore_index=True) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.6467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRT0R-zhP96l",
        "colab_type": "text"
      },
      "source": [
        "#### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kuEikm5GPQx4",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "param_grid = {}\n",
        "grid = GridSearchCV(GaussianNB(max), param_grid, cv=5, scoring='f1_macro')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cW1f1aIIPQyA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1992cc6c-6c95-45ff-dcc8-50abc7306322"
      },
      "source": [
        "## CountVectorizer\n",
        "\n",
        "Cvect_nb = CountVectorizer(analyzer='word')\n",
        "X_train_cvect_nb= Cvect.fit_transform(text_train)\n",
        "X_test_cvect_nb = Cvect.transform(text_test)\n",
        "\n",
        "nb_cv = cross_val_score(GaussianNB(), X_train_cvect_nb.toarray(), y_train, cv=5, scoring='f1_macro')\n",
        "print(\"Mean Cross-validation f1_macro score: {:.4f}\".format(np.mean(nb_cv)))\n",
        "allmodelscorelist = allmodelscorelist.append({'PreProcessing':'CountVectorizer','ML_Model':'Naive Bayes','Mean_Score':np.mean(nb_cv)}, ignore_index=True) "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Cross-validation f1_macro score: 0.5560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3rkeI2aRjsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "a16196e0-c350-434b-ec24-eb6451cca541"
      },
      "source": [
        "allmodelscorelist = allmodelscorelist.drop_duplicates()\n",
        "print(allmodelscorelist)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       PreProcessing            ML_Model Mean_Score\n",
            "0    CountVectorizer  LogisticRegression     0.7163\n",
            "1    TfidfVectorizer  LogisticRegression     0.6810\n",
            "2        Bag of Word  LogisticRegression     0.7163\n",
            "3  HashingVectorizer  LogisticRegression     0.6648\n",
            "4    CountVectorizer   XGBoostClassifier     0.7151\n",
            "5    TfidfVectorizer   XGBoostClassifier     0.6897\n",
            "6        Bag of Word   XGBoostClassifier     0.6991\n",
            "7  HashingVectorizer   XGBoostClassifier     0.6467\n",
            "8    CountVectorizer         Naive Bayes     0.5560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO9WnSvGONj7",
        "colab_type": "text"
      },
      "source": [
        "Suprisingly!! Kaggle submitted, best score is without pre processing the data!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlkZVx2OMcKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "14ef9b66-8784-4623-8435-72d242da0fd2"
      },
      "source": [
        "##Kaggle Submitted Model\n",
        "text_train_kaggle, y_train = train_data['text'], train_data['Target']\n",
        "text_test_kaggle = test_data['text']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "Cvect = CountVectorizer(analyzer='word', ngram_range=(1,2), max_features=80000)\n",
        "X_train_cvect= Cvect.fit_transform(text_train_kaggle)\n",
        "X_test_cvect = Cvect.transform(text_test_kaggle)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "param_grid = {'C': [0.8,0.9,1.0,1.1,1.2]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200,penalty= 'l2'), param_grid, cv=5, scoring='f1_macro')\n",
        "Kaggle_lg_cv = grid.fit(X_train_cvect, y_train)\n",
        "print(\"Best cross-validation f1_macro score: {:.4f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best cross-validation f1_macro score: 0.7305\n",
            "Best parameters:  {'C': 1.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMs_Bn_6b9df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = grid.predict(X_test_cvect)\n",
        "y_id=np.arange(7322, 14642, 1) \n",
        "output = pd.DataFrame(y_pred,y_id)\n",
        "output.to_excel(\"/content/drive/My Drive/Colab Notebooks/NLP/HW1/train/output1.0.xlsx\")"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}